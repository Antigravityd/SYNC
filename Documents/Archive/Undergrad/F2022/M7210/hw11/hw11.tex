\documentclass{article}

\usepackage[letterpaper]{geometry}
\usepackage{tgpagella}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{tikz}
\usepackage{minted}
\usepackage{physics}
\usepackage{siunitx}

\sisetup{detect-all}
\newtheorem{plm}{Problem}
\DeclareMathOperator{\Ann}{Ann}

\title{7210 HW 11}
\author{Duncan Wilkie}
\date{22 November 2022}

\begin{document}

\maketitle

\begin{plm}[D\&F 12.1.4]
  Let $R$ be an integral domain, let $M$ be an $R$-module, and let $N$ be a submodule of $M$.
  Suppose $M$ has rank $n$, $N$ has rank $r$, and the quotient $M / N$ has rank $s$.
  Prove that $n = r + s$.
\end{plm}

\begin{proof}
  Let $x_{1}, x_{2}, \ldots, x_{s}$ be representatives of the maximal set of linearly independent cosets in $M / N$,
  and let $x_{s+1}, x_{s+2}, \ldots, x_{s+r}$ be a maximal set of linearly independent elements in $N$.
  We show that $x_{1}, x_{2}, \ldots, x_{s}, x_{s+1}, x_{s+2}, \ldots, x_{s+r}$ is a maximal set of linearly independent elements in $M$.
  Linear independence in $M / N$ means no nonzero linear combination of representatives of the cosets can ever be in $N$;
  since, by the submodule axioms, all linear combinations of $x_{s+1}, x_{s+2}, \ldots, x_{s+r}$ must be in $N$,
  this means that $a_{1}x_{1} + a_{2}x_{2} + \cdots + a_{s}x_{s} + a_{s+1}x_{s+1} + \cdots + a_{s+r}x_{s+r}$ is of the form $m + n$ for some nonzero
  $m \in M$, $n \in N$.
  If $m + n = 0 \Leftrightarrow m = -n$, then $a_{1}x_{1} + a_{2}x_{2} + \cdots + a_{s}x_{s} \in N$ by closure of $N$ under negation,
  contradicting linear independence of $x_{1}, x_{2}, \ldots, x_{s}$.
  This large list is therefore certainly linearly independent

  For any nonzero $x_{0} \in M$, consider the linear combination
  \[
    a_{0}x_{0} + a_{1}x_{1} + \cdots + a_{s}x_{s} + a_{s+1}x_{s+1} + \cdots + a_{s+r}x_{s+r}.
  \]
  By maximal linear independence of the cosets represented by $x_{1}, x_{2}, \ldots, x_{s},$
  there must exist some $a_{0}$ (likely dependent on $a_{1}, a_{2}, \ldots, a_{s}$) such that $a_{0}x_{0} + a_{1}x_{1} + \cdots + a_{s}x_{s} \in N$.
  This linear combination is therefore of the form
  \[
    n + a_{s+1}x_{s+1} + a_{s+2}x_{s+2} + \cdots + a_{s+r}x_{s+r}
  \]
  for $n \in N$; by maximal linear independence of $x_{s+1}, x_{s+2}, \ldots, x_{s+r}$ in $N$
  and the fact that linear independence is preserved under scalar multiplication in any module over an integral domain,
  there must exist $a', a_{s+1}', a_{s+2}', \ldots, a_{s+r}'$ such that
  \[
    a'n + a_{s+1}'a_{s+1}x_{s+1} + a_{s+2}'a_{s+2} + \cdots + a_{s+r}'a_{s+r}x_{s+r} = 0
  \]
  Expanding the definition of $n$ and distributing, one obtains an $R$-linear combination of \newline
  $x_{0}, x_{1}, \ldots, x_{s}, x_{s+1}, \ldots, x_{s+r}$
  that evaluates to zero, so the linearly independent set above is maximal.
  This proves $n = r + s$.
\end{proof}

\begin{plm}[D\&F 12.1.6]
  Show that if $R$ is an integral domain and $M$ is any non-principal ideal of $R$ then $M$ is torsion-free of rank 1 but is not a free $R$-module.
\end{plm}

\begin{proof}
  $M$ is torsion-free since $R$ is an integral domain: $rm = 0$ for nonzero $r, m$ can't hold by definition.
  Accordingly, $M$ has rank at least 1, since any nonzero element is linearly independent.
  For any two elements $n, m \in M$, one can take the $R$-linear combination $nm + (-m)n = 0$ to show that the rank is precisely 1.

  If $M$ were free, then any element would be able to be uniquely written as a linear combination of basis elements,
  i.e. for all $m \in M$, $m = rm_{0}$ uniquely for unique $r \in R$ varying alongside $m$ and fixed $m_{0} \in M$.
  However, this would mean that $M$ is a principal ideal, as every element would be a multiple of $m_{0}$, so $M$ can't be free.
\end{proof}

\begin{plm}[D\&F 12.1.11]
  Let $R$ be a P.I.D., let $a$ be a nonzero element of $R$ and let $M = R / (a)$.
  For any prime $p$ of $R$ prove that
  \[
    p^{k-1}M / p^{k}M \cong
    \begin{cases}
      R / (p) & \text{if } k \leq n \\
      0 & \text{if } k > n,
    \end{cases}
  \]
  where $n$ is the power of $p$ dividing $a$ in $R$.
\end{plm}

\begin{proof}
  This is proven via induction on $k$ with the isomorphism theorems.
  $p^{k}M = p^{k}(R / (a))$ has elements of the form $p^{k}r + (a)$; the left term is precisely the form of elements of $(p^{k})$,
  so the submodule is of the form $[(p^{k}) + (a)] / (a)$.
  The numerator ideal is generated by the greatest common divisor of $p^{k}$ and $a$, which, if $p^{k} \mid a$ (i.e. $k \leq n$), is $p^{k}$,
  and otherwise is $p^{n}$.
  Therefore, $p^{k}M = (p^{k}) / (a)$ if $k \leq n$ and $p^{k}M = (p^{n}) / (a)$ otherwise.
  Analogously, $p^{k-1}M = (p^{k-1}) / (a)$ if $k-1 < k \leq n$ and $p^{k-1}M = (p^{n}) / a$ otherwise.
  The factor module is then $[(p^{k-1}) / (a)] / [(p^{k}) / (a)] \cong (p^{k-1}) / (p^{k}) \cong (1) / (p) \cong R / (p)$ if $k \leq n$
  by the third isomorphism theorem.
  If $k - 1 = n$, so $k > n$, the factor module is $[(p^{k-1}) / (a)] / [(p^{n}) / (a)] \cong (p^{k-1}) / (p^{n}) \cong (1) / (1) \cong 0$
  If $k > n$, then the factor module is $[(p^{n}) / (a)] / [(p^{n}) / (a)] \cong (1) / (1) \cong 0$.
\end{proof}

\begin{plm}[D\&F 12.3.19]
  Prove that all $n \times n$ matrices with characteristic polynomial $f(x)$ are similar iff $f(x)$ has no repeated factors
  in its unique factorization over $F[x]$.
\end{plm}

\begin{proof}
  Two matrices are similar iff they have the same rational canonical form.
  If $f(x)$ has repeated irreducible factors, say $f(x) = p_{1}^{\epsilon_{1}}p_{2}^{\epsilon_{2}} \cdots p_{m}^{\epsilon_m}$ where some $\epsilon_i > 1$,
  then the matrices whose rational canonical forms are constructed from the invariant factors
  \[
    p_{i} \mid p_{1}^{\epsilon_{1}}p_{2}^{\epsilon_{2}} \cdots p_{i}^{\epsilon_{i} - 1} \cdots p_{m}^{\epsilon_m}
  \]
  and
  \[
    p_{1}^{\epsilon_{1}}p_{2}^{\epsilon_{2}} \cdots p_{i}^{\epsilon_{i}} \cdots p_{m}^{\epsilon_m}
  \]
  are not similar, despite having the same characteristic polynomials.

  If $f(x)$ has no repeated factors, then it must be only one irreducible factor: since each invariant factor must divide the next,
  and they are multiplied to yield $f(x)$, one has $a_{i} = ka_{i-1}$ so the product includes $a_{i-1}(ka_{i-1})$ which is a repeated factor
  of at least whatever irreducibles are inside $a_{i-1}$.
  Accordingly, if $f(x)$ has no repeated factors, the only possible rational canonical form is that with the single, irreducible invariant factor.
  Such matrices are then necessarily similar.
\end{proof}

\begin{plm}[D\&F 12.3.21]
  Show that if $A^{2} = A$ then $A$ is similar to a diagonal matrix which has only $0$'s and $1$'s along the diagonal.
\end{plm}

\begin{proof}
  If $A = 0$, we're done; the matrix is diagonalized.
  Similarly if $A = I$.
  Therefore, since $A^{2} = A$ implies $m_{T}(A) \mid x^{2} - x = x(x - 1)$, the minimal polynomial of $A$ is $x^{2} - x$.
  This has no repeat factors, so the Jordan canonical form of $A$ is diagonal.
  Since this diagonal form also satisfies the same minimal polynomial, $\lambda(\lambda - 1) = 0$ for all diagonal entries,
  i.e. all diagonal entries are either 0 or 1.
\end{proof}
\end{document}
