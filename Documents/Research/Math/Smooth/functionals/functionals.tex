\documentclass{article}

\usepackage[letterpaper]{geometry}
\usepackage{tgpagella}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{tikz}
\usepackage{minted}
\usepackage{physics}
\usepackage{siunitx}

\sisetup{detect-all}
\newtheorem{plm}{Problem}
\newtheorem{thm}{Theorem}
\newtheorem{definition}{Definition}



\title{Functionals for Smooth Infinitesimals}
\author{Duncan Wilkie}
\date{25 January 2023}

\begin{document}

\maketitle

\section{Abstract}

The smooth infinitesimal analysis introduced by Lawvere and Kock, among others,
through intense search for the proper topos for differential geometry, has promise as a better axiomatic implementation of the continuum.
Chief among the reasons are the theory's computational simplicity, the indivisibility of the continuum, and constructive (and therefore computable)
foundations---these are all exhibited in Bell's excellent monograph on the subject\footnote
{
  Which, incidentally, is of such unparalleled quality to make me wish analytic philosophers wrote more pedagogical mathematics.
}.
One aspect that seems, at first glance, to impede its universal physical application is that \textit{one cannot define discontinuous functions.}
This paper argues that  discontinuous functions that seem to arise in physics are actually never best regarded as functions at all.
Using this insight, applicable to the ordinary reals, we tease out an analog in smooth worlds, and apply it to solve several \textit{prima facie}
discontinuous physical situations.

\section{Physics Needs to Differentiate at Discontinuities}

Consider the following example from Griffiths' \textit{Introduction to Electromagnetism}.

\begin{plm}
  Imagine a very long solenoid with radius $R$, $n$ turns per unit length, and current $I$.
  Coaxial with the solenoid are two long cylindrical (non-conducting) shells of length $l$---one, \textit{inside} the solenoid at radius $a$,
  carries a charge $+Q$, uniformly distributed over its surface; the other, \textit{outside} the solenoid, at radius $b$,
  carries charge $-Q$ (with $l \gg b$).
  When the current in the solenoid is gradually reduced, the cylinders begin to rotate.
  Where does the angular momentum come from? % TODO: figure
\end{plm}

\begin{proof}[Solution 1]
  Initially, the fields are stationary: the electric field from the charges is (by Gauss's law, in the natural cylindrical coordinates)
  \[
    \vec{E} = \frac{Q}{2\epsilon_{0}l}\frac{1}{r}\hat{r} \; (a < s < b)
  \]
  and the magnetic is (by Ampere's law and some literal hand-waving)
  \[
    \vec{B} = \mu_{0}nI\hat{z} \; (r < R)
  \]
  When the current decreases, Faraday's law tells us that the changing magnetic field induces an electric field:
  \[
    \oint_{\partial S} \vec{E} \cdot d\vec{\ell} = -\iint_{S} \pdv{B}{t} \cdot d\vec{S}
  \]
  Taking $S$ to be a disk slicing the whole cylindrical setup perpendicular to the common axis, one can chug out
  \[
    E =
    \begin{cases}
      -\frac{1}{2}\mu_{0}n\dv{I}{t}\frac{R^{2}}{s}\hat{\phi}, & (s > R) \\
      -\frac{1}{2}\mu_{0}n\frac{dI}{dt}s\hat{\phi}, & (s < R)
    \end{cases}
  \]
  It's this electric field, tangent to the surface of the shells, that torques the cylinders in opposite directions;
  indeed this is what's physically observed.
\end{proof}

\begin{proof}[Solution 2]
  With the same setup as before, apply Faraday's law in its equivalent form:
  \[
    \curl{\vec{E}} = -\pdv{\vec{B}}{t}
  \]
  Pointwise, there's never a magnetic field outside the solenoid, \textit{so there is no curl in the electric field outside the solenoid!}
  Namely, this means that there's no net perpendicular component to the electric field at the outer charged cylinder---and therefore no rotation!
\end{proof}

What gives?
The only difference is that we've changed which form of Maxwell's equations we're working with; they're supposed to be equivalent, right?
The physicist's resolution of the paradox is to call it a failure of the idealized approximation: certainly, in any ``very long solenoid,''
there's some part of the magnetic field (however weak) that circles back to the other end,
and it's probably just some jiggery-pokery with this mismatch causing the issue.
However, this dodges the real meat of the issue: the integral formalism \textit{just works}.
A careful inspection of why the two forms of Maxwell's equations are (usually) equivalent reveals the error.

The theorem we're implicitly using when sensing a contradiction in these arguments is Stokes'.

\begin{thm}
  Let $S$ be a smooth, oriented (2-dimensional) surface in $\mathbb{R}^{3}$ with boundary $\partial S$.
  If a vector field $\vec{F}(x, y, z)$ is defined and has continuous first-order partials in a region containing $S$, then
  \[
    \iint_{S}(\curl{\vec{F}}) \cdot d\vec{A} = \oint_{\partial S}\vec{F} \cdot d\vec{\ell}.
  \]
\end{thm}

Immediately, we see that the primary hypothesis fails---$\vec{B}$ jumps to 0 at the boundary of the solenoid.
The takeaway is that the ``more physical'' version of Maxwell's equations are the integral forms; it's a strictly more general formalism.
Indeed, this justifies many other subtle arguments deep in the weeds of E\&M, such as the interface conditions for the fields when exiting media.

Is this just a quirk of electromagnetism?
No---in fact, it's a much more general pattern in physics that key mathematically-tractable examples are \textit{fundamentally} discontinuous.
There's a canonical example from quantum mechanics where the potential isn't even a \textit{function}!
The Dirac potential is a fundamental model for particle-particle scattering and strong-force capture.

The unifying mathematical machinery can be obtained by synthesizing the opposing intuition of both the physicist and the mathematician.
The physicist notices that such examples arise from application of approximation, and so posits \textit{limits} of smooth functions
as the proper mathematical grounding; his dreams contain results asymptotically bounding such approximations,
giving \textit{quantitative} limits on his scribblings' predictive power, and, should he be so daring, renormalization-free QFT.
The mathematician (or philosopher) notices that the \textit{raison d'etre} of these objects never was their being \textit{functions} anyway.
They exist to \textit{act} on smooth functions.
For quantum potentials, this is especially obvious; for both electromagnetic fields and distributions of their sources,
it requires noticing that \textit{trajectories} of charged particles are the fundamental observed quantites,
and that fields and even sources are deduced from how trajectories are modified by their presence\footnote
{
  If the lack of a physically measurable distinction between sources and the field they introduce bothers you,
  note that it's already implicit in the terminology!
  A source is nothing more than ``that inducing a field,'' and a field is nothing more than ``that producing motion at a distance.''
  Review the great experimentalists' work if doubts remain; physics is the study of motion, after all,
  and it's only through analysis of motion that we deduce the physical existence of \textit{anything.}
}.
These trajectories are, by assumptions held for half a millenium, smooth; one has no guarantee that the phenomena influencing them are,
and certainly not that the mere function that happens to represent the desired function-of-functions via some analytical gadget is.
The mathematician dreams of vector spaces, of alternative models for $L^{2}(\mathbb{C})$ and Sobolev space.

Nestruev noticed that the restriction of physical observation in a lab naturally places a smooth-manifold structure on anything classical.
His excellent text develops the modern, sheaf-theoretic notions of differential geometry using this motivation.
We'll adopt his viewpoint that a physics lab is a commutative, unital algebra; the sensors in that lab are elements of the algebra;
physical states are algebra homomorphisms onto the trivial algebra structure on the reals;
and sensor outputs are the image of an element under the homomorphism.
Such structure, although much more general than the standard definition, certainly deserves the title of ``smooth manifold.''
Accordingly, it's plausible that the most natural context in which to do physical mathematics is,
rather than set-theoretical orthodoxy, those topoi painstakingly constructed to temper the agony of dealing with such objects\footnote
{
  Explained at a bird's-eye level in Bell, these were developed by Lawvere and Kock in an attempt to find places where
  one might embed the category of manifolds, fully and faithfully.
  As my use of the plural might indicate, there is no unique such object, and in fact a healthy spectrum of theories and models.
  By all means, window-shop; some of the stronger ones have fancy features like the ability to emulate nonstandard analysis,
  another formulation of infinitesimal calculus that realizes infinitesimals as reciprocals of transfinite cardinals.
}.

Bell considers the image of the reals in these so-called smooth worlds.
The calculus so-constructed is simple, computational, and delightfully geometric---perfectly regular, no $\epsilon$ or $\delta$
(save those whose square is zero), and straightforward proofs.
There \textit{aren't} non-smooth functions, though.
How does one recover the myriad discontinuities of physical mathematics?

\section{Recovering Discontinuities}

A formalism for generalized functions that's excellent even for ordinary analysis is that presented by Egorov\footnote
{
  It's primary advantage from that perspective is its algebraic manners.
  With, say, the standard Schwartz theory, one cannot obtain an associative product of distributions,
  despite successful application of such intuition in physical contexts.
  Additionally, it has the rare feature of being both more general and \textit{dead} simple---for a contrast on both counts,
  see the elegant-but-complicated fruits of the Sato school.
  From a physical and philosophical perspective, Egorov presents many delightful examples
  of how his notion coincides with physical observations, and presents extensions of distributional theory
  to nonlinear contexts where past constructions failed.
}.
It's all readily reinterpretable in a smooth world, in one of which all subsequent notation ought to be interpreted (in particular,
$\mathbf{R}$ is the continuum of smooth infinitesimal analysis, and $\mathbf{C}$ is a field extension of it by roots of $x^{2} + 1$).

The first step is to define generalized complex numbers: take the compactification of $\mathbf{C}$ by a point $\infty$,
and consider the set of equivalence classes of sequences in this compactification that are eventually the same.
That is, identify $a_{n} \equiv b_{n}$ iff there exists $N$ such that $a_{k} = b_{k}$ for all $k > N$.
This construction, with the obvious pairwise structure making it an algebra, is termed the \textbf{extended complex plane},
and any of its elements is a \textbf{generalized complex number}.

Analogously, one may consider all sequences of functions on a domain $\Omega$ in $\mathbf{R}^{n}$ (\textit{a fortiori} smooth functions),
and identify sequences that are eventually the same on every compact subset,
i.e $f_{k} \equiv g_{k}$ iff for each compact $K \subseteq \Omega$ there exists $N$ such that $f_{k}(x) = g_{k}(x)$ for all $k > N, x \in K$.
This identified set is denoted $\mathcal{G}(\Omega)$, and its elements are called \textbf{generalized functions}.





\begin{definition}
  The space $F_{c}(U)$ of compactly-supported functions on $U$ is, for any open subset $U \subseteq R^{n}$,
  the vector space of all functions $f: U \to R$ such that $\{x \in U \mid f(x) \neq 0\}$ is contained in some closed hypercube in $R^{n}$,
  with addition and scalar multiplication defined pointwise.
\end{definition}

\begin{definition}
  A distribution on $U \subseteq R^{n}$ is a function $T: F_{c}(U) \to \mathbb{R}$
  that is linear ($\forall v, w \in V: T(v + w) = T(v) + T(w)$)
  and homogeneous ($\forall v \in V \forall \alpha \in \mathbb{R}: T(\alpha v) = \alpha T(v)$).
\end{definition}

The deviation from the classical approach is that we don't require distributions to be continuous.

In classical analysis, many distributions are justifiably confused with functions $\mathbb{R} \to \mathbb{R}$
by means of the Reisz representation theorem,
whose realization for function spaces of interest allows one to identify many functionals with a partially applied inner product.
Such confusion is far less generally possible in smooth worlds---often, the function fixed in the inner product is discontinuous.
Whether this is a bug or a feature depends on whether the computational expedient is valued more than mandatory type-transparency.

\end{document}
